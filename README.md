# NexusCore - MCP OLLAMA RAG AGENT - ðŸ”Search, ðŸ“Š Analysis and âœï¸ Generate from sources

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Un agent conversationnel intelligent implÃ©mentant le protocole MCP (Machine Conversation Protocol) avec capacitÃ©s RAG (Retrieval-Augmented Generation) utilisant Ollama comme backend LLM.


## PrÃ©sentation de l'application

Cette application est un agent conversationnel intelligent qui combine plusieurs technologies avancÃ©es :

1. **Protocole MCP** : Une implÃ©mentation du standard de communication machine-to-machine pour les agents IA, permettant une intÃ©gration standardisÃ©e avec d'autres systÃ¨mes.

2. **RAG avec Ollama** : 
   - Utilisation de modÃ¨les LLM locaux via Ollama
   - Pipeline complet de traitement des documents (extraction, dÃ©coupage, embedding)
   - Recherche vectorielle avec FAISS (CPU/GPU)

3. **Architecture multi-agents** :
   - Agent de recherche (combine recherche web et RAG)
   - Agent d'analyse (analyse sÃ©mantique et statistique)
   - Agent de gÃ©nÃ©ration (crÃ©ation de contenu)

4. **FonctionnalitÃ©s avancÃ©es** :
   - Logging structurÃ© pour le dÃ©bogage
   - Configuration centralisÃ©e
   - Support multi-fournisseurs de recherche (Exa, Firecrawl)
   - Gestion automatique des erreurs

L'application est particuliÃ¨rement adaptÃ©e pour :
- Les assistants virtuels intelligents
- Les systÃ¨mes de recherche d'information augmentÃ©e
- Les outils d'analyse de contenu
- Les gÃ©nÃ©rateurs de contenu automatisÃ©s

Le systÃ¨me est conÃ§u pour Ãªtre extensible avec de nouveaux agents et capacitÃ©s tout en maintenant une architecture propre et modulaire.

## FonctionnalitÃ©s clÃ©s

- ðŸ“š**Recherche augmentÃ©e** : Combinaison de recherche web et RAG pour des rÃ©ponses prÃ©cises
- ðŸ¤– **Multi-agents** : Architecture modulaire avec agents spÃ©cialisÃ©s
- âœ…**Protocole MCP** : ImplÃ©mentation du standard de conversation machine-to-machine
- âœ¨**IntÃ©gration Ollama** : Utilisation des modÃ¨les LLM locaux via Ollama
- ðŸ“**Traitement avancÃ©** : Extraction, nettoyage et analyse de contenu web

## Architecture technique

```markdown
NexusCore/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agent.py                 # Main agents (research, analysis, generation)
â”‚   â”œâ”€â”€ agent_orchestrator.py    # Agent coordination logic
â”‚   â”œâ”€â”€ config.py                # Central configuration
â”‚   â”œâ”€â”€ mcp_server.py            # FastAPI MCP implementation
â”‚   â”œâ”€â”€ rag.py                   # RAG processing
â”‚   â”œâ”€â”€ search.py                # Advanced web search
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logging_service.py   # Structured logging service
â”‚
â”œâ”€â”€ pyproject.toml               # Project configuration
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ .env.sample                  # Environment template
```

## PrÃ©requis

- Python 3.10+
- Ollama installÃ© localement avec au moins un modÃ¨le (ex: `llama3`)
- ClÃ© API pour un fournisseur de recherche (Exa ou Firecrawl)


## Installation

0. Ollama
->  Installation sur votre systeme (Unix ou Windows) (https://ollama.com/)
->  Charger les modeles LLMs
```bash
# Exemple: Chargement du modele "mistral"
ollama pull mistral

# Lancement du serveur sur une session terminal indÃ©pendante.
ollama serve
```

1. Cloner le dÃ©pÃ´t :
```bash
git clone https://github.com/votre-repo/ollama-rag-agent.git
cd ollama-rag-agent
```

2. Installer les dÃ©pendances :
->  Creer l'environnement avec uv (plus rapide et fiable)
```bash
uv venv .venv
.venv\scripts\activate # Windows
#ou 
source .venv\scripts\activate # Unix
#puis
uv pip install -r requirements.txt
```

3. Configurer l'environnement de l'application et ses paramÃ¨tres :
```bash
# Ã‰diter le fichier .env puis ajoutez vos configurations (basÃ©e sur  .env sample)
cp .env.sample .env #Unix
edit .env           #Windows

```

## Utilisation
Lancer le serveur MCP

### Lancer le serveur mcp
```bash
cd app
uv run mcp_server.py
```
Le serveur sera accessible sur http://localhost:8000 avec les endpoints suivants :

    . POST /search - Recherche augmentÃ©e
    . POST /analyze - Analyse de texte
    . POST /generate - GÃ©nÃ©ration de contenu
    .  GET /health - VÃ©rification de l'Ã©tat du serveur


### Utilisation en ligne de commande:

```bash
python agent.py "Votre requÃªte ici"
```
ou 

```bash
uv run agent.py "Votre requÃªte ici"
```

## Configuration

Les paramÃ¨tres principaux sont configurables via le fichier .env :
```bash
# ModÃ¨le Ollama
OLLAMA_MODEL=llama3.2
OLLAMA_BASE_URL=http://localhost:11434

# ParamÃ¨tres RAG
RAG_CHUNK_SIZE=4096
RAG_CHUNK_OVERLAP=512

# Fournisseur de recherche
SEARCH_PROVIDER=exa  # ou firecrawl
EXA_API_KEY=votre_cle_api
```

## Licence

Ce projet est sous licence MIT - voir le fichier LICENCE.md pour plus de dÃ©tails.

## Auteur

Olivier Lavaud - 2025


# SchÃ©ma MCP : Fonctionnement Central

```flowchart TB
    subgraph MCP_Core["ðŸ”· MCP Server (FastAPI)"]
        A[Endpoint /mcp] --> B[Auth & Validation]
        B --> C[Task Decomposition]
        C --> D[Agent Selection]
        D --> E[Parallel Execution]
        E --> F[Result Aggregation]
    end

    subgraph Agents["âš™ï¸ Agents Modules"]
        E --> G[ResearchAgent]
        E --> H[AnalysisAgent]
        E --> I[GenerationAgent]
    end

    subgraph External["ðŸŒ Externals"]
        G --> J[(Web APIs)]
        H --> K[(Vector DB)]
        I --> L[LLM API]
    end

    F --> M[Response Formatting]
    M --> N[/Client/]
```

## Flux Critique :

    DÃ©coupage des tÃ¢ches : Le MCP analyse la requÃªte pour identifier les sous-tÃ¢ches.
    *Exemple : "Recherche sur X + analyse comparative + synthÃ¨se" â†’ 3 jobs parallÃ¨les*.

    SÃ©lection dynamique : Utilise le config.py pour router vers les agents compÃ©tents.

    ExÃ©cution parallÃ¨le : Chaque agent travaille isolÃ©ment avec son propre contexte.

    AggrÃ©gation intelligente : Fusion des rÃ©sultats partiels avec gestion des conflits.

## SÃ©quence API Typique

```sequenceDiagram
    participant C as Client
    participant M as MCP
    participant R as ResearchAgent
    participant A as AnalysisAgent

    C ->> M: POST /mcp {query: "Comparer React et Svelte"}
    M ->> R: Task: "Recherche React"
    M ->> A: Task: "Recherche Svelte"
    R ->> M: Results (React)
    A ->> M: Results (Svelte)
    M ->> M: Cross-Analysis
    M ->> C: 200 OK {comparison: [...]}
```

# Technical Documentation
## System Purpose

MCP-RAG-Ollama serves as a versatile query answering system that combines the power of large language models with real-time information retrieval. It enables users to:

    Get answers to questions using contextual knowledge from web searches
    Implement RAG workflows with Ollama models
    Access the system via both API endpoints and command-line interface



## Architecture Overview
The system follows a layered architecture pattern, separating client interactions, server operations, core processing, and external service integrations.
<p align="center"><img src="img/i1.png" alt="[Architecture Overview" width="600" height="400"></p>

## Core Components
The system consists of four main components that work together to provide RAG capabilities:
<p align="center"><img src="img/i2.png" alt="[Core Components" width="600" height="400">
</p>

```bash
    - MCP Server: The FastAPI server that exposes endpoints for client interactions.
    - OllamaAgent: The orchestrator that processes queries by combining web search and RAG.
    - RAG Processor: Handles document processing, embedding generation, and similarity search.
    - Web Searcher: Performs web searches and content extraction from relevant pages.
```

## Query Processing Flow
The following diagram illustrates how a user query flows through the system:

<p align="center"><img src="img/i3.png" alt="[Core Components" width="600" height="400"></p>

## Technology Stack

The MCP-RAG-Ollama system leverages multiple technologies and libraries:

<p align="center"><img src="img/i5.png" alt="[Technology Stack" width="500" height="300"></p>

## Deployment Architecture
The system can be deployed as follows:*

<p align="center"><img src="img/i4.png" alt="Deployment Architecture" width="600" height="400"></p>
