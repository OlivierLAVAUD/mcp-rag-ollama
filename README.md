# Ollama RAG Analysis Agent

Un agent intelligent qui combine recherche web, RAG (Retrieval-Augmented Generation) et analyse LLM pour fournir des rÃ©ponses enrichies avec Ã©valuation automatique de qualitÃ©.

## FonctionnalitÃ©s clÃ©s

- ğŸ” **Recherche web avancÃ©e** avec extraction des contenus pertinents
- ğŸ“š **IntÃ©gration RAG** pour contextualiser les rÃ©ponses
- ğŸ¤– **Analyse automatique** des rÃ©ponses par LLM (Ollama)
- âœ… **Ã‰valuation de qualitÃ©** : pertinence, cohÃ©rence, biais, clartÃ©
- âœ¨ **RÃ©sumÃ© automatique** des points clÃ©s
- ğŸ“ **Gestion transparente des sources** avec tracking complet

## PrÃ©requis

- Python 3.9+
- Ollama installÃ© et configurÃ© (serveur local ou distant)
- ModÃ¨le LLM compatible (par dÃ©faut: `mistral`)

## Installation

0. Ollama
```bash
# Charger les modeles
ollama pull mistral
# Lancer le serveur
ollama serve
```

1. Cloner le dÃ©pÃ´t :
```bash
git clone https://github.com/votre-repo/ollama-rag-agent.git
cd ollama-rag-agent
```

2. Creer l'environnement et installer les packages python 
```bash
uv venv .venv
.venv\scripts\activate # Windows
source .venv\scripts\activate # Unix

uv pip install -r requirements.txt
```

3. Lancer le serveur mcp
```bash
cd app
uv run mcp_server.py
```
##   Utiliser l'agent en CLI :

```bash
python agent.py "Votre question ici"
```
ou 

```bash
uv run agent.py "Votre question ici"
```