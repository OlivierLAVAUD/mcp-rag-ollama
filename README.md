# Ollama RAG Analysis Agent

Un agent intelligent qui combine recherche web, RAG (Retrieval-Augmented Generation) et analyse LLM pour fournir des r√©ponses enrichies avec √©valuation automatique de qualit√©.

## Fonctionnalit√©s cl√©s

- üîç **Recherche web avanc√©e** avec extraction des contenus pertinents
- üìö **Int√©gration RAG** pour contextualiser les r√©ponses
- ü§ñ **Analyse automatique** des r√©ponses par LLM (Ollama)
- ‚úÖ **√âvaluation de qualit√©** : pertinence, coh√©rence, biais, clart√©
- ‚ú® **R√©sum√© automatique** des points cl√©s
- üìù **Gestion transparente des sources** avec tracking complet

## Pr√©requis

- Python 3.9+
- Ollama install√© et configur√© (serveur local ou distant)
- Mod√®le LLM compatible (par d√©faut: `llama3`)

## Installation

0. Ollama
```bash
# Charger les modeles
ollama pull mistral
# Lancer le serveur
ollama serve
```

1. Cloner le d√©p√¥t :
```bash
git clone https://github.com/votre-repo/ollama-rag-agent.git
cd ollama-rag-agent
```

2. Creer l'environnement et installer les packages python 
```bash
uv venv .venv
.venv\scripts\activate # Windows
source .venv\scripts\activate # Unix

uv pip install -r requirements.txt
```

3. Lancer le serveur mcp
```bash
cd app
uv run mcp_server.py
```
4. Executer l'app sur une autre session ou terminal 
```bash
cd app
.venv\scripts\activate # Windows
source .venv\scripts\activate # Unix
# Exemple
uv run agent.py "Qui est Jean Moulin?"
# la reponse dans le terminal et le fichier response_analysis.txt
```

##   Utiliser l'agent en CLI :

```bash
python agent.py "Votre question ici"
```
ou 

```bash
uv run agent.py "Votre question ici"
```