# MCP RAG Agent avec Ollama

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Un agent conversationnel intelligent impl√©mentant le protocole MCP (Machine Conversation Protocol) avec capacit√©s RAG (Retrieval-Augmented Generation) utilisant Ollama comme backend LLM.

## Fonctionnalit√©s cl√©s

- üìö**Recherche augment√©e** : Combinaison de recherche web et RAG pour des r√©ponses pr√©cises
- ü§ñ **Multi-agents** : Architecture modulaire avec agents sp√©cialis√©s
- ‚úÖ**Protocole MCP** : Impl√©mentation du standard de conversation machine-to-machine
- ‚ú®**Int√©gration Ollama** : Utilisation des mod√®les LLM locaux via Ollama
- üìù**Traitement avanc√©** : Extraction, nettoyage et analyse de contenu web

## Architecture technique
```bash
.
‚îú‚îÄ‚îÄ app/
‚îÇ ‚îú‚îÄ‚îÄ agent.py # Agents principaux (recherche, analyse, g√©n√©ration)
‚îÇ ‚îú‚îÄ‚îÄ agent_orchestrator.py # Orchestration des agents
‚îÇ ‚îú‚îÄ‚îÄ config.py # Configuration centrale
‚îÇ ‚îú‚îÄ‚îÄ mcp_server.py # Serveur FastAPI impl√©mentant MCP
‚îÇ ‚îú‚îÄ‚îÄ rag.py # Traitement RAG
‚îÇ ‚îú‚îÄ‚îÄ search.py # Recherche web avanc√©e
‚îÇ ‚îî‚îÄ‚îÄ utils/
‚îÇ ‚îî‚îÄ‚îÄ logging_service.py # Service de logging structur√©
‚îú‚îÄ‚îÄ pyproject.toml # Configuration du projet
‚îú‚îÄ‚îÄ requirements.txt # D√©pendances Python
‚îî‚îÄ‚îÄ .env.sample # Configuration d'environnement
```
## Pr√©requis

- Python 3.10+
- Ollama install√© localement avec au moins un mod√®le (ex: `llama3`)
- Cl√© API pour un fournisseur de recherche (Exa ou Firecrawl)


## Installation

0. Ollama
->  Installation sur votre systeme (Unix ou Windows) (https://ollama.com/)
->  Charger les modeles LLMs
```bash
# Exemple: Chargement du modele "mistral"
ollama pull mistral

# Lancement du serveur sur une session terminal ind√©pendante.
ollama serve
```

1. Cloner le d√©p√¥t :
```bash
git clone https://github.com/votre-repo/ollama-rag-agent.git
cd ollama-rag-agent
```

2. Installer les d√©pendances :
->  Creer l'environnement avec uv (plus rapide et fiable)
```bash
uv venv .venv
.venv\scripts\activate # Windows
#ou 
source .venv\scripts\activate # Unix
#puis
uv pip install -r requirements.txt
```

3. Configurer l'environnement de l'application et ses param√®tres :
```bash
# √âditer le fichier .env puis ajoutez vos configurations (bas√©e sur  .env sample)
cp .env.sample .env #Unix
edit .env           #Windows

```

4. Lancer le serveur mcp
```bash
cd app
uv run mcp_server.py
```
5. Utiliser l'agent en CLI :

```bash
python agent.py "Votre question ici"
```
ou 

```bash
uv run agent.py "Votre question ici"
```


# Technical Documentation

## System Purpose

MCP-RAG-Ollama serves as a versatile query answering system that combines the power of large language models with real-time information retrieval. It enables users to:

    Get answers to questions using contextual knowledge from web searches
    Implement RAG workflows with Ollama models
    Access the system via both API endpoints and command-line interface



## Architecture Overview
The system follows a layered architecture pattern, separating client interactions, server operations, core processing, and external service integrations.
<p align="center"><img src="img/i1.png" alt="[Architecture Overview" width="600" height="400"></p>

## Core Components
The system consists of four main components that work together to provide RAG capabilities:
<p align="center"><img src="img/i2.png" alt="[Core Components" width="600" height="400">
</p>

```bash
    - MCP Server: The FastAPI server that exposes endpoints for client interactions.
    - OllamaAgent: The orchestrator that processes queries by combining web search and RAG.
    - RAG Processor: Handles document processing, embedding generation, and similarity search.
    - Web Searcher: Performs web searches and content extraction from relevant pages.
```

## Query Processing Flow
The following diagram illustrates how a user query flows through the system:

<p align="center"><img src="img/i3.png" alt="[Core Components" width="600" height="400"></p>

## Technology Stack

The MCP-RAG-Ollama system leverages multiple technologies and libraries:

<p align="center"><img src="img/i5.png" alt="[Technology Stack" width="500" height="300"></p>

## Deployment Architecture
The system can be deployed as follows:*

<p align="center"><img src="img/i4.png" alt="Deployment Architecture" width="600" height="400"></p>
