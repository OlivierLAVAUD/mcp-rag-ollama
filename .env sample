
# === Génération Configuration du Modèle LLM via Ollama ===
OLLAMA_MODEL=llama3.2
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=llama3.2

# La température à 0.7 permet une certaine variété tout en maintenant la sensibilité, tandis que Top-P à 0.75 assure une bonne diversité dans les choix de mots. Le nombre de tokens moyen (1024) convient à la plupart des applications générales.
OLLAMA_MODEL_TEMPERATURE=0.7  # Contrôle la créativité (0-1)
OLLAMA_MODEL_MAX_TOKENS=1024  # Nombre maximum de tokens générés
OLLAMA_MODEL_TOP_P=0.75       # Filtrage par noyau (0-1)


# === Configuration FAISS ===
FAISS_TYPE=cpu  # ou 'gpu' si disponible
FAISS_INDEX_PATH=./storage/faiss_index

# === Paramètres RAG ===
RAG_CHUNK_SIZE=4096
RAG_CHUNK_OVERLAP=512
RAG_RESULTS=3

# === Configuration Recherche ===
SEARCH_PROVIDER=exa  # exa ou firecrawl
SEARCH_TIMEOUT=30
SEARCH_MAX_RESULTS=5
SEARCH_AUTOPROMPT=true
SEARCH_API_KEY=

# === API Keys ===
EXA_API_KEY=
FIRECRAWL_API_KEY=

# === Configuration Serveur MCP ===

SERVER_NAME=MCP A2A Server for LLM RAG SEARCH
SERVER_DESCRIPTION=Serveur MCP implémentant le protocole Machine Conversation Protocol
SERVER_VERSION=1.0.0
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
SERVER_DEBUG=false
SERVER_WORKERS=4
SERVER_LOG_LEVEL=info

# === Configuration Logging ===
LOGGING_DIR=./logs
LOGGING_MAX_SIZE=15
LOGGING_BACKUP_COUNT=5
LOGGING_ENCODING=utf-8



